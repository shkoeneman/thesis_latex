\addtocontents{toc}{\protect\vspace{\li}}
\addcontentsline{toc}{head}{APPENDIX}
\renewcommand{\theequation}{A.\arabic{equation}}
\setcounter{equation}{0}
\begin{center}
\textbf{APPENDIX}
\end{center}
\doublespace
\phantom{a}
\phantom{a}
\noindent

\section*{Modified Model Selection Algorithm for Normal Linear Regression Using the F-Test}

Here, a modification to the model selection procedure for AIC presented in Chapter 3 is developed that relies on fewer asymptotic properties.
This modified procedure provided only marginal benefit in small sample sizes in terms of prediction and description at the cost of much
greater computational complexity, and thus is only presented here for completeness.

We make many of the same assumptions we did in developing the procedure in Chapter 3, mainly that all candidates are of the same parametric
class, that there is some largest candidate model in which all others are nested, and that this largest candidate model does not exhibit
profound lack of fit. However, we additionally assume that all models are normal linear regression models. Thus, nested models differ
from the largest candidate model in that they do not include as many covariates.

We begin once again by observing the difference in AIC between the largest candidate model $M_*$ and some nested candidate $M_i$. This difference
will have the familiar form
\begin{equation}
	AIC_i - AIC_* = \left( -2 \ell (\hat{\theta}_*) + 2 \ell (\hat{\theta}_i) \right) - 2k_i ,
\end{equation}
where $k_i$ is once again the difference in number of estimated parameters between model $M_*$ and model $M_i$. Recall that the likelihood
goodness-of-fit term for a normal linear regression model can be expressed as
\begin{equation}
	-2 \ell (\hat{\theta}) = n \log(2 \pi) + n + n \log(\hat{\sigma} ^2 ) ,
\end{equation}
where $\hat{\sigma} ^2$ is the estimated error variance of a fitted linear regression model. Applying this relation to our difference if goodness-of-fit
terms, and denoting the estimated error variance for model $M_*$ as $\hat{\sigma}_* ^2$ and for model $M_i$ as $\hat{\sigma^2_i}$, we can see that
\begin{equation}
	\begin{split}
	-2 \ell (\hat{\theta}_*) + 2 \ell (\hat{\theta}_i) & = \\ 
	& = (n\log(2\pi) + n + n\log(\hat{\sigma}^2_i)) + (-n\log(2\pi) - n - n\log(\hat{\sigma}^2_*)) \\
	& = n\log(\frac{\hat{\sigma}^2_i}{\hat{\sigma}^2_*}) .
	\end{split}
\end{equation}
Thus, applying this equivalence to our AIC difference, we see that
\begin{equation}
	AIC_i - AIC_* = n\log(\frac{\hat{\sigma}^2_i}{\hat{\sigma}^2_*}) - 2k_i .
\end{equation}
Now, we consider the F-Test for nested normal linear regression models. This test has a null hypothesis that the nested model fits just as well as the
larger model, or equivalently, that the true value of the additional effects present in the larger model is 0 (Allen, 1998). Let us denote the sum
of squared errors for model $M_*$ as $SSE_*$ and for model $M_i$ as $M_i$. The form and distribution for the test statistic $F_i$ of the F-Test 
performed between model $M_*$ and model $M_i$ would, under the null hypothesis, be
\begin{equation}
	\frac{\frac{SSE_i - SSE_*}{k_i}}{\frac{SSE_*}{n-(p+k_i)}} = F_i \sim F_{k_i, n-(p+k_i)} ,
\end{equation}
where $p$ is the number of estimated parameters in model $M_*$.

Now note that by definition, $SSE_i = n \hat{\sigma}^2_i$, and similarly, $SSE_* = n \hat{\sigma}^2_*$. Using these relations, we can re-express
$F_i$ as
\begin{equation}
	F_i = \frac{\frac{n \hat{\sigma}^2_i - n \hat{\sigma}^2_*}{k_i}}{\frac{n \hat{\sigma}^2_*}{n-(p+k_i)}} ,
\end{equation}
which will still have the F distribution presented above under the null hypothesis. Performing algebraic manipulation on this equality and
then taking the log of each side, we see that
\begin{equation}
	\log( \frac{k_i}{n-p-k_i} F_i + 1 ) = \log(\frac{\hat{\sigma}^2_i}{\hat{\sigma}^2_*}) .
\end{equation}
Thus, the quantity $F_*$, which is known to have an exact F distribution under the F-Test null hypothesis, can be shown to be transformed to
be equivalent to the form of the difference in likelihood goodness-of-fit terms in the case of nested normal linear regression models.
It is then natural to express the difference in AIC values as
\begin{equation}
	AIC_i - AIC_* = n \log( \frac{k_i}{n-p-k_i} F_i + 1 ) - 2k_i
\end{equation}
Therefore, under the F-Test null hypothesis, one knows that the exact distribution of this AIC difference will be a transformation of the
distribution of $F_i$. If one knew this distribution and its properties, one could standardize $AIC_i - AIC_*$ as was done in Chapter 3
to have unit variance $\forall i$, and additionally formulate an informed cutoff as was done in Chater 3 using the likelihood ratio
test properties.

However, the transformation applied to $F_i$ is complex, and does not result in any commonly known distribution. Therefore to estimate
the mean and variance of $AIC_i - AIC_*$, we propose taking many samples from an F distribution distributed the same as $F_i$
using software, applying the transformation including the constant of $-2k_i$ to each sampled value, and then empirically estimating
a mean and variance. This can be done for each distinct $k_i$, as the distribution will be the same for models with the same difference
in number of estimated parameters from the reference model. In this way, if enough samples are taken, one can obtain precise estimates
of the exact mean and variance of $AIC_i - AIC_*$.

Using the variance estimates, one could then standardize $AIC_i - AIC_*$ $\forall i$. Additionally, one could take the largest estimated
mean and use this quantity plus 2 as the cutoff, similar to what was done in the model selection procedure presented in Chapter 3. This
procedure would be very similar in formulation, but would not rely on the asymptotic properties of the likelihood-ratio test, instead
relying on the exact F-Test and a Monte Carlo simulation to obtain a reasonable estimate for the exact mean and variance of the difference
in criterion values. Furthermore, while it will not be presented here, this modification could be applied to work with other likelihood-based
information criteria with constant penalty terms as well with minor tweaks.

While this procedure is intriguing, implementation of it in practice has shown that the benefits in small sample sizes appear to only be
marginal as compared to the likelihood ratio test version of the procedure. Additionally, this procedure can be far more computationally
intestive as a Monte Carlo simulation needs to be performed for each distinct $k_i$ to determine distributional properties, and a sufficient
number of iterations in each simulation must be done to achieve acceptable precision. This procedure is also limited in scope to selecting
from among a collection of normal linear regression models.

Thus, for general use, the procedure in Chapter 3 is to be recommended. However, this modification using the F-Test is presented for the
sake of completeness.


