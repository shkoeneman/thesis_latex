\addtocontents{toc}{\protect\vspace{\li}}
\addcontentsline{toc}{head}{APPENDIX}
\renewcommand{\theequation}{A.\arabic{equation}}
\setcounter{equation}{0}
\begin{center}
\textbf{APPENDIX}
\end{center}
\doublespace
\phantom{a}
\phantom{a}
\noindent

\section*{Modified Model Selection Algorithm for Normal Linear Regression Using the F-Test}

Here, a modification to the model selection procedure for AICc presented in Chapter 3 is developed that allows one to rely on no asymptotic properties.
This modified procedure provided only marginal benefit in small sample settings in terms of prediction and description at the cost of much
greater computational complexity, and thus is only presented here for completeness.

We make many of the same assumptions we did in developing the general procedure in Chapter 3, mainly that all candidates are of the same parametric
family, that there is some largest candidate model in which all others are nested, and that this largest candidate model does not exhibit
profound lack of fit. We additionally assume that all models are normal linear regression models, as was the case for the algorithm for
AICc. Thus, nested models differ from the largest candidate model in this scenario that they do not include as many covariates.

We begin once again by observing the difference in AICc between the largest candidate model $M_*$ and some nested candidate $M_i$. This difference
will have the familiar form
\begin{equation*}
	\begin{split}
	AICc_i - AICc_* = \left( -2 \ell (\hat{\theta}_*) + 2 \ell (\hat{\theta}_i) \right) + \\
	\left[ 2(p-k_i) \left( \frac{n}{n-p+k_i-1} \right) - 2p \left( \frac{n}{n-p-1} \right) \right] ,
	\end{split}
\end{equation*}
where $k_i$ is once again the difference in number of estimated parameters between model $M_*$ and model $M_i$, and $p$ is the total number of
estimated parameters in model $M_*$. Recall that the likelihood goodness-of-fit term for a normal linear regression model can be expressed as
\begin{equation*}
	-2 \ell (\hat{\theta}) = n \log(2 \pi) + n + n \log(\hat{\sigma} ^2 ) ,
\end{equation*}
where $\hat{\sigma} ^2$ is the estimated error variance of a fitted linear regression model. Applying this relation to our difference in goodness-of-fit
terms, and denoting the estimated error variance for model $M_*$ as $\hat{\sigma}_* ^2$ and for model $M_i$ as $\hat{\sigma^2_i}$, we can see that
\begin{equation*}
	\begin{split}
	-2 \ell (\hat{\theta}_*) + 2 \ell (\hat{\theta}_i) & = (n\log(2\pi) + n + n\log(\hat{\sigma}^2_i)) + (-n\log(2\pi) - n - n\log(\hat{\sigma}^2_*)) \\
	& = n\log \left( \frac{\hat{\sigma}^2_i}{\hat{\sigma}^2_*} \right) .
	\end{split}
\end{equation*}
Thus, applying this equivalence to our AIC difference, we see that
\begin{equation*}
	\begin{split}
	AICc_i - AICc_* = n\log \left( \frac{\hat{\sigma}^2_i}{\hat{\sigma}^2_*} \right) + \\
	\left[ 2(p-k_i) \left( \frac{n}{n-p+k_i-1} \right) - 2p \left( \frac{n}{n-p-1} \right) \right] .
	\end{split}
\end{equation*}
Now, we consider the F-Test for nested normal linear regression models. This test has a null hypothesis that the nested model fits just as well as the
larger model, or equivalently, that the true value of the additional effects present in the larger model are 0 (Allen, 1998). Let us denote the sum
of squared errors for model $M_*$ as $SSE_*$ and for model $M_i$ as $SSE_i$. The form and distribution for the test statistic $F_i$ of the F-Test 
performed between model $M_*$ and model $M_i$ would, under the null hypothesis, be
\begin{equation*}
	\frac{\frac{SSE_i - SSE_*}{k_i}}{\frac{SSE_*}{n-(p+k_i)}} = F_i \sim F_{k_i, n-(p+k_i)} ,
\end{equation*}
where $p$ is the number of estimated parameters in model $M_*$.

Now note that by definition, $SSE_i = n \hat{\sigma}^2_i$, and similarly, $SSE_* = n \hat{\sigma}^2_*$. Using these relations, we can re-express
$F_i$ as
\begin{equation*}
	F_i = \frac{\frac{n \hat{\sigma}^2_i - n \hat{\sigma}^2_*}{k_i}}{\frac{n \hat{\sigma}^2_*}{n-(p+k_i)}} ,
\end{equation*}
which will still have the F distribution presented above under the null hypothesis. Performing algebraic manipulation on this equality and
then taking the log of each side, we see that
\begin{equation*}
	\log \left( \frac{k_i}{n-p-k_i} F_i + 1 \right) = \log \left( \frac{\hat{\sigma}^2_i}{\hat{\sigma}^2_*} \right) .
\end{equation*}
Thus, the quantity $F_i$, which is known to have an exact F distribution under the F-Test null hypothesis, can be shown to be transformed to
be equivalent to the form of the difference in likelihood goodness-of-fit terms in the case of nested normal linear regression models.
It is then natural to express the difference in AICc values as
\begin{equation*}
	\begin{split}
	AICc_i - AICc_* = n \log \left( \frac{k_i}{n-p-k_i} F_i + 1 \right) + \\
	\left[ 2(p-k_i) \left( \frac{n}{n-p+k_i-1} \right) - 2p \left( \frac{n}{n-p-1} \right) \right] .
	\end{split}
\end{equation*}
Therefore, under the F-Test null hypothesis, one knows that the exact distribution of this AICc difference will be a transformation of the
distribution of $F_i$. If one knew this distribution and its properties, one could standardize $AICc_i - AICc_*$ as was done in Chapter 3
to have unit variance $\forall\: i$, and additionally formulate an informed cutoff as was done in Chater 3 using the likelihood ratio
test properties.

However, the transformation applied to $F_i$ is complex, and does not result in any commonly known distribution. Therefore to estimate
the mean and variance of $AICc_i - AICc_*$, we propose taking many samples using software from an F distribution distributed the same as $F_i$,
applying the transformation including the constant of $\left[ 2(p-k_i) \left( \frac{n}{n-p+k_i-1} \right) - 2p \left( \frac{n}{n-p-1} \right) \right]$ to each sampled value,
and then empirically estimating a mean and variance. This estimation can be done for each distinct $k_i$, as the distribution will be the same for models with
the same difference in number of estimated parameters from the reference model. In this way, if enough samples are taken, one can obtain precise estimates
of the exact mean and variance of $AICc_i - AICc_*$.

Using the variance estimates, one could then standardize $AICc_i - AICc_*$ $\forall\: i$. Additionally, one could take the largest estimated
mean and use this quantity plus 2 as the cutoff, similar to what was done in the model selection procedure presented in Chapter 3. This
procedure would be very similar in formulation, but would not rely on the asymptotic properties of the likelihood-ratio test, instead
relying on the exact F-Test and a Monte Carlo simulation to obtain a reasonable estimate for the exact mean and variance of the difference
in criterion values. Furthermore, while it will not be presented here, this modification could be applied to work with other likelihood-based
information criteria with constant penalty terms as well with minor tweaks.

While this procedure is intriguing, implementation of it in practice has shown that the benefits in small sample sizes appear to only be
marginal as compared to the likelihood ratio test version of the procedure. Additionally, this procedure can be far more computationally
intensive as a Monte Carlo simulation to determine distributional properties needs to be performed for each distinct $k_i$, and a sufficient
number of iterations in each simulation must be done to achieve acceptable precision. This procedure is also limited in scope to selecting
from among a collection of normal linear regression models, even if applied to a criterion other than AICc, as the F-Test properties only hold
for such models.

Thus, for general use, the procedure in Chapter 3 is to be recommended. However, this modification using the F-Test is presented for the
sake of completeness.


\section*{Exact Variance of the Likelihood Goodness-of-Fit Term for a Correctly Specified Linear Model}

This section will derive an exact variance for the goodness-of-fit term $-2 \ell (\hat{\theta} )$ in the case of a properly specified
linear model. We assume that this linear regression model has a full rank $n$ by $r$ design matrix $X$, outcome vector $y$, true parameters $\beta$ and $\sigma^2$,
and maximum likelihood estimates $\hat{\beta}$ and $\hat{\sigma}^2$. It can be shown that the estimate $\hat{\sigma}^2$ can be expressed as
\begin{equation*}
	\hat{\sigma}^2 =  \frac{1}{n} (y-X\hat{\beta})'(y-X\hat{\beta}) = \frac{1}{n} y'(I_n - X(X'X)^{-1}X')y ,
\end{equation*}
where $I_n$ is an $n$ by $n$ identity matrix. We let
\begin{equation*}
	A = \frac{1}{\sigma^2} (I_n - X(X'X)^{-1}X') 
\end{equation*}
such that we can construct the quadratic form
\begin{equation*}
	y'Ay = \frac{n \hat{\sigma}^2}{\sigma^2} .
\end{equation*}
Let $V = \sigma^2 I_n$ be the covariance matrix of the random multivariate normal outcome vector $y$, and note that $AV$ is idempotent. Let $\mu = X \beta$ be the
true mean of the outcome vector $y$. By the properties of quadratic forms of multivariate normal random variables (Rencher and Schaalje, 2008), it can be seen that
\begin{equation*}
	y'Ay = \frac{n \hat{\sigma}^2}{\sigma^2} \sim \chi^2_{rank \left( A \right) } (\mu 'A\mu) ,
\end{equation*}
where $rank \left( A \right)$ is the degrees of freedom and $\mu 'A \mu$ is the non-centrality parameter of this chi-squared distribution.

Note that, as we assume $X$ is full rank, $rank(X) = r$. Furthermore, it can be seen that
\begin{equation*}
	\begin{split}
	tr\left[ X(X'X)^{-1}X' \right] & = tr \left[ X'X(X'X)^{-1} \right]  \\ 
	& = tr \left[ I_r \right] \\
	& = r .
	\end{split}
\end{equation*}
Thus, noting that $\sigma^2 A$ is idempotent, it can be seen that
\begin{equation*}
	\begin{split}
	tr \left[ \sigma^2 A \right] & = tr \left[ I_n - X(X'X)^{-1}X' \right]  \\ 
	& = n-r \\
	& = rank \left( \sigma^2 A \right) .
	\end{split}
\end{equation*}
Therefore, as $rank \left( \sigma^2 A \right) = rank \left( A \right)$ since $\sigma^2$ is a scalar constant, then $rank \left( A \right) = n-r$. This value will be the degrees of
freedom in the chi-squared distribution presented above.

Now, we observe the non-centrality parameter $\mu 'A \mu = (X \beta)'A(X \beta)$. Note that
\begin{equation*}
	\begin{split}
	A(X \beta) & = \frac{1}{\sigma^2} (I_n - X (X' X)^{-1} X')X\beta  \\ 
	& = \frac{1}{\sigma^2} (X \beta - X \beta) \\
	& = 0 .
	\end{split}
\end{equation*}
Thus, by extension $\mu 'A \mu = 0$, and this distribution is a central chi-squared distribution.

We have shown that
\begin{equation}
	\frac{n \hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n-r} .
\end{equation}
Note that if we take the natural logarithm of the left-hand side and take its variance, we see that
\begin{equation}
	\begin{split}
		Var \left[ \log(\frac{n \hat{\sigma}^2}{\sigma^2}) \right] & = Var \left[ \log n + \log \hat{\sigma}^2 - \log \sigma^2 \right]  \\ 
		& = Var \left[ \log \hat{\sigma}^2 \right] \\
	\end{split}
\end{equation}
due to constant terms being irrelevant to the variance calculation. This enclosed form is very close to the $n \log \hat{\sigma}^2$ that is required
to determine the variance of the likelihood goodness of fit term $-2 \ell (\hat{\theta} )$ in the case of a normal linear regression model.
Thus, if we can calculate $Var \left[ \log \hat{\sigma}^2 \right]$, which is itself the variance
of the natural logarithm of a variable distributed as $\chi^2_{n-r}$, we can easily obtain $Var \left[ n \log \hat{\sigma}^2 \right]$.

Thus, we now derive the variance of the logarithm of a central chi-squared random variable. Let $W \sim \chi^2_{\nu}$, and let $Y = \log(W)$. Note that
the moment generating function of $Y$ can be expressed as
\begin{equation*}
	M_Y (t) = E \left[ e^{yt} \right] = E \left[ e^{t\log(w)} \right] = E \left[ e^{\log w^t} \right] = E \left[ w^t \right] .
\end{equation*}
Thus, the moment generating function of $Y$ evaluated at $t$ is the $t^{th}$ moment of $W$. The moments of $W$ will have the
closed form (Lancaster, 1969) of
\begin{equation*}
	E \left[ w^t \right] = 2^t \frac{\Gamma (t + \frac{\nu}{2})}{\Gamma (\frac{\nu}{2})} = E \left[ e^{yt} \right] = M_Y (t) .
\end{equation*}
We will use this relation to find the moments of $Y$, the logarithm of a central chi-squared random variable.

Taking the first derivative with respect to $t$, we see that
\begin{equation*}
	\frac{\partial M_Y(t)}{\partial t} = (2^t \log 2) \frac{\Gamma (t + \frac{\nu}{2})}{\Gamma (\frac{\nu}{2})} + 2^t \frac{\Gamma ' (t + \frac{\nu}{2})}{\Gamma (\frac{\nu}{2})} .
\end{equation*}
Note that in general $\frac{\Gamma ' (z)}{\Gamma (z)} = \psi^{(0)}(z)$, where $\psi^{(0)}(z)$ is the digamma function. With this relation in mind, we see that the first moment of
$Y$ can be calculated as
\begin{equation*}
	E \left[ Y^1 \right] =  \left. \frac{\partial M_Y(t)}{\partial t} \right|_{t=0} = \log 2 + \psi^{(0)} \left( \frac{\nu}{2} \right) .
\end{equation*}
Thus, we have a form for the first moment of the logarithm of a central chi-squared random variable.

Now moving on to calculating the second moment of $Y$, we take the next derivative of the moment generating function with respect to $t$ and see that
\begin{equation*}
	\begin{split}
		\frac{\partial^2 M_Y(t)}{\partial t^2} = (\log 2)^2 2^t  \frac{\Gamma (t + \frac{\nu}{2})}{\Gamma (\frac{\nu}{2})} + 2^{t+1} \log 2 \frac{\Gamma ' (t + \frac{\nu}{2})}{\Gamma (\frac{\nu}{2})} + \\ 
		2^t \frac{\Gamma '' (t + \frac{\nu}{2})}{\Gamma (\frac{\nu}{2})} .\\
	\end{split}
\end{equation*}
This above expression evaluated at $t = 0$ can be simplified by introducing the trigamma function $ \psi^{(1)}(z)$. Noting that the trigamma function is the derivative with respect
to $z$ of the digamma function $\psi^{(0)}(z) = \frac{\Gamma ' (z)}{\Gamma (z)}$, it can be seen that
\begin{equation*}
	\psi^{(1)}(z) = \frac{\partial \psi^{(0)}(z)}{\partial z} = \frac{\Gamma (z) \Gamma '' (z) - \Gamma '(z) \Gamma '(z)}{(\Gamma(z))^2} .
\end{equation*}
By re-arranging this relation, we see that
\begin{equation*}
	\Gamma ''(z) = \psi^{(1)}(z) \Gamma(z) + \frac{(\Gamma '(z))^2}{\Gamma(z)} .
\end{equation*}
Thus, moving to calculate the second moment of $Y$, we see that
\begin{equation*}
	E \left[ Y^2 \right] =  \left. \frac{\partial^2 M_Y(t)}{\partial t^2} \right|_{t=0} =
	(\log 2)^2 + \psi^{(0)} \left( \frac{\nu}{2} \right) 2 \log 2  + \frac{\Gamma '' (\frac{\nu}{2})}{\Gamma (\frac{\nu}{2})} .
\end{equation*}
Using the identity established above involving the trigamma function, we see that this moment can be expressed as
\begin{equation*}
	E \left[ Y^2 \right] =
	(\log 2)^2 + \psi^{(0)} \left( \frac{\nu}{2} \right) 2 \log 2  + \psi^{(1)} \left( \frac{\nu}{2} \right) + \left( \psi^{(0)} \left( \frac{\nu}{2} \right) \right)^2 .
\end{equation*}

Thus, with both the first and second moments of $Y$ in hand, we may calculate the variance. The variance of $Y$, the log of a central chi-squared random variable with
$\nu$ degrees of freedom, is found to be
\begin{equation*}
	Var \left[ Y \right ] = E \left[ Y^2 \right] - \left( E \left[ Y \right] \right)^2 = \psi^{(1)} \left( \frac{\nu}{2} \right) .
\end{equation*}
The variance will simply be the trigamma function evaluated at half of the degrees of freedom. This value can be calculated using software to approximate the trigamma function,
although it should be noted that this calculation may be unstable for certain values of the degrees of freedom.

Recalling the distributional result established in (A.1) and relation in (A.2), it is clear that
\begin{equation*}
	Var \left[ \log \hat{\sigma}^2 \right] = \psi^{(1)} \left( \frac{n-r}{2} \right)
\end{equation*}
based on the variance that has been derived above. Multiplying each side by $n^2$, we see that
\begin{equation*}
	Var \left[ n \log \hat{\sigma}^2 \right] = Var \left[ -2 \ell (\hat{\theta} ) \right] = n^2 \psi^{(1)} \left( \frac{n-r}{2} \right).
\end{equation*}
Thus, we have established the exact variance of the likelihood goodness-of-fit term in the case of a correctly specified linear model. This exact variance is in contrast to the
approximate variance found in Chapter 4.

However, while this variance is exact, it still involves an approximation as the trigamma function must be approximated. Additionally, it was found that
this variance provided no benefit when used in the bootstrap goodness-of-fit procedure developed in Chapter 4. Thus, this derivation is presented here only
for completeness and as a matter of theoretical interest.










