\doublespace
\Chapter{BACKGROUND}
		In this chapter, various likelihood-based information criteria and goodness-of-fit assessments are presented and some of their properties are also briefly introduced.
		These two areas provide the basis for the model selection algorithm and goodness-of-fit procedure that will be presented later in the thesis. Additionally, theory
		related to the likelihood ratio test, robust sandwich variance estimators, and the bootstrap will be touched upon as these constructs will be relevant to developments
		related to model selection and goodness-of-fit.
		
		\section{Likelihood-Based Information Criteria}

		Let us introduce some notation that will serve us for the remainder of this thesis. Say that one has an $n$ by 1  vector of observations $y$ and wishes to formulate a
		statistical model for these outcomes. We will call this model our \textit{candidate} model, and we will assume it belongs to a \textit{parametric class} of probability distributions.
		Let the parameters for the class in question be denoted as $\theta$, a vector of size $p$ by 1. Thus, all possible models of the same parametric class can be denoted
		as
		\begin{equation*}
			\mathcal{F}(p) = \left\{ f(y|\theta) | \theta \in \Theta(p) \right\} ,
		\end{equation*}
		where $\Theta(p)$ is the parameter space composed of $p$-dimensional vectors that contain functionally independent components.

		We will assume that $y$ was generated by some unknown probability density $g(y)$; we will refer to $g(y)$ as the \textit{true} model. For a given candidate model $f(y|\theta)$
		and a known true model $g(y)$, one measure of the similarity between the two models is the \textit{Kullback-Leibler information} (Kullback and Leibler, 1951). The Kullback-Leibler
		information between $f(y|\theta)$ and $g(y)$ with respect to the true model is
		\begin{equation*}
			I(\theta) = E \left[ \log \frac{g(y)}{f(y|\theta)} \right],
		\end{equation*}
		where the expectation is taken with respect to the true model $g(y)$. While the Kullback-Leibler information is not a distance measure, it has some of the same properties in
		that the larger the separation between $g(y)$ and $f(y|\theta)$, the larger $I(\theta)$ will be in general (Cavanaugh and Neath, 2019).
		
		Next, let us define the \textit{Kullback-Leibler discrepancy} as
		\begin{equation*}
			d(\theta) = E[-2 \log f(y|\theta)] .
		\end{equation*}
		Note that this quantity can be related to the Kullback-Leibler information as
		\begin{equation*}
			2 I(\theta) = d(\theta) - E \left[ -2 \log g(y) \right].
		\end{equation*}
		This relation shows that any dependence of the Kullback-Leibler information on $\theta$ is captured by the Kullback-Leibler discrepancy, as the two only differ by a positive
		multiplicative constant of 2 and a term that only depends on $g(y)$. Thus, ranking models of the same class, but with different parameter values, using the Kullback-Leibler
		discrepancy will be equivalent to doing so using the Kullback-Leibler information (Kullback, 1968).
		
		Let a $p$ by 1 vector of the estimates for the $\theta$ parameters be denoted as $\hat{\theta}$. These would be obtained through the process of fitting a candidate model in the
		parametric class at hand, perhaps by using the method of maximum likelihood. Evaluating the Kullback-Leibler discrepancy at these estimated values of the parameters, we obtain
		\begin{equation*}
			d(\hat{\theta}) = E [ -2 \log f(y | \theta)]|_{\theta = \hat{\theta}}.
		\end{equation*}

		This quantity would give us a theoretical measurement of the separation between our fitted candidate model $f(y|\hat{\theta})$ and the true model $g(y)$. However, to
		evaluate this expectation requires knowledge of the true generating model $g(y)$. In practice, in any situation where we are concerned with selecting and fitting models,
		we will not possess knowledge of this true model.

		However, while this exact quantity is inaccessible in practical applications, one may find that its expectation, $E[d(\hat{\theta})]$, can be estimated. An
		effort to do so is what led to the formulation of the \textit{Akaike information criterion}, colloquially known as \textit{AIC}. For a fitted model $f(y|\hat{\theta})$,
		AIC is evaluated as
		\begin{equation*}
			AIC = -2 \log f(y|\hat{\theta}) + 2 p,
		\end{equation*}
		with this statistic serving as an asymptotically unbiased estimator for the expected Kullback-Leibler discrepancy subject to appropriate assumptions being met (Akaike, 1974). Specifically, AIC was derived
		under the assumption that the true generating model $g(y)$ belongs to the parametric class $\mathcal{F}(p)$, and thus the property of being asymptotically unbiased only
		holds in this scenario. Being an estimator of the expected Kullback-Leibler discrepancy, smaller values of AIC for models therefore indicate smaller discrepancies of candidate
		models from the generating model.  

		One may notice several features of the form of AIC. Using $\ell(\hat{\theta}|y)$ to denote the log-likelihood of the fitted model, AIC can equivalently be expressed as
		\begin{equation*}
			AIC = -2 \ell(\hat{\theta}|y) + 2 p.
		\end{equation*}
		The $-2 \ell(\hat{\theta}|y)$ term based on the empirical log-likelihood is known as the \textit{goodness-of-fit} term as it represents the degree to which the fitted model
		conforms to the observed data $y$ (Cavanaugh and Neath, 2019). This term will only grow smaller as complexity is added to the model. Note that this quantity is
		a statistic as it depends on the data $y$, and the estimate $\hat{\theta}$ which was determined using the data $y$ itself.

		The second term of AIC, that being $2 p$, is commonly referred to as the \textit{penalty} term. This term serves as a bias correction to the goodness-of-fit term to form an asymptotically
		unbiased estimator, as the likelihood-based goodness-of-fit term on its own is biased (Cavanaugh and Neath, 2019). This term also serves to penalize models with more fitted parameters and
		thus more complexity, as this term will grow with $p$ and cause higher values of AIC. Thus, the goodness-of-fit term and penalty term serve to harmonize and provide tradeoffs between
		conformity to data, which can only increase as more parameters are added, versus parsimony, that being simplicity and frugality with regards to including extraneous, unnecessary
		parameters.

		As noted prior, AIC is only asymptotically unbiased. The penalty term of AIC only serves as a sufficient bias correction as the sample size $n$ tends toward infinity. It is well-documented that
		with small sample sizes when this asymptotic property does not hold, even as an approximation, that AIC can be substantially biased (Brewer et al., 2016). This has led to the development of the 
		\textit{corrected Akaike information criterion}, generally denoted \textit{AICc}. AICc was first proposed by Sugiura as a means to develop an exactly unbiased estimator for the expected
		Kullback-Leibler discrepancy of a fitted normal linear regression model, thus allowing AICc to be safely used in a similar manner to AIC with small sample sizes (Sugiura, 1978). The form of AICc is 
		\begin{equation*}
			AICc = -2 \ell(\hat{\theta}|y) + \frac{2 r n}{n - r - 1},
		\end{equation*}
		where $r$ in this case specifically denotes the number of estimated regression coefficients in the model. Note that while the goodness-of-fit term matches exactly the goodness-of-fit term
		of AIC, the penalty term here has been adjusted to provide an exact bias adjustment in the case of  normal linear regression. Since AICc was first derived, work has been done that unifies
		the justification of AIC and AICc in the linear regression framework such that it becomes clear how the two criteria are developed in a parallel fashion (Cavanaugh, 1997). AICc will converge
		to AIC in large sample sizes, as the penalty term for AICc will converge to $2p$, but in small sample sizes, AICc can provide better results than AIC when used for model selection
		(Burnham and Anderson, 2003). However, one must be mindful that AICc is derived with a specific parametric family in mind and does not have the broad generalizability of AIC in any singular form.

		Although AICc was initially derived only for normal linear regression models, it has been extended to other parametric families since that time. AICc was first extended to non-linear regression
		and autoregressive frameworks a bit over a decade after it was first formulated for normal linear regression (Hurvich and Tsai, 1989). This was soon followed by an extension to autoregressive
		moving-average models (Hurvich et al., 1990). AICc has subsequently been extended to various other modeling frameworks including vector autoregressive models (Hurvich and Tsai, 1993), generalized
		linear models with a dispersion parameter (Hurvich and Tsai, 1995), and longitudinal models with a known covariance structure (Azari et al., 2006). These justifications of AICc follow the same general
		pattern as the initial derivation for normal linear regression, in that while the goodness-of-fit terms remains the same, the penalty term is adjusted to provide an improved bias adjustment.

		Not all likelihood-based information criteria were formulated as estimators of a discrepancy measure. The \textit{Bayesian information criterion}, often denoted as \textit{BIC}, looks similar in
		form to AIC as it has the likelihood goodness-of-fit term in addition to a penalty term. It is defined for a fitted model as
		\begin{equation*}
			BIC = -2 \ell(\hat{\theta}|y) + p \log (n).
		\end{equation*}
		BIC was first derived and justified in the case of the regular exponential family to select the model which is \textit{a posteriori} most probable in the Bayesian sense in
		large-sample settings (Schwarz, 1978). Models with lower BIC are to be considered more probable. BIC has since been generalized to justify its use in other modeling frameworks beyond
		the case of models from the regular exponential family (Cavanaugh and Neath, 1999). It it worth noting that while BIC bears resemblance to AIC, it does not seek to estimate any discrepancy
		measure, and rather provides a large-sample estimator of a transformation of the Bayesian posterior probability associated with the candidate model.

		Note that when $n$ is relatively larger than $p$, the penalty term of BIC will be much more stringent than that of AIC, while the goodness-of-fit term remains the same. This gives BIC a tendency to
		choose models that are more parsimonious than AIC when performing selection on the same group of candidate models (Neath and Cavanaugh, 2011). BIC is also fundamentally different
		from AIC in that it is an \textit{asymptotically consistent} criterion,  whereas AIC is an \textit{asymptotically efficient} criterion. An asymptotically consistent criterion will asymptotically
		select the fitted model having correct structure with probability one, provided the generating model is of a finite dimension and is in the set of candidate models. An asymptotically efficient
		criterion will asymptotically select the fitted candidate model which minimizes the mean squared error of prediction with probability one, even if the generating model is of infinite dimension
		or outside of the set of candidate models.

		Thus, it has been espoused that AIC and the variant AICc are to be preferred when \textit{prediction} is of primary importance, and that BIC is to be preferred
		when \textit{description} is of chief importance when performing model selection. Simulation studies have shown that this recommendation has some merit (Neath and Cavanaugh, 2011). However, these
		ideas of consistency and efficiency only hold in the asymptotic sense, and choosing the model with the minimum criterion value provides no guarantee that the model possesses the traits of either
		property in finite sample sizes.
		
		Additionally, each criterion has sampling variability in finite sample sizes. Thus, if selecting a model with the minimum criterion value is the method of choice, this may lead to a model that is
		deficient in prediction or description by chance. This consideration has lead to the development of rules of thumb for using these criteria beyond simply choosing the minimum AIC, BIC, or AICc model.

		As stated in the introduction, Burnham and Anderson (2002) proposed the recommendations which can be found in Table 1.1 for using AIC in finite sample settings. This paradigm states that models
		that are within 2 units of AIC of the minimum AIC model are not to be discounted. Thus, if there are multiple models that are within this threshold, then it may be prudent to treat each of them
		as a valid candidate for selection, and select a model based on parsimony, scientific intuition, or some other method. However, the thresholds presented in Table 1.1 were created using empirical
		observation, and do not have any apparent probabilistic justification behind them.

		Kass and Raftery (1995) developed similar recommendations for BIC, producing Table 2.1.
		\begin{table}[H]
		\centering
		\ttabbox[\FBwidth]
		{\caption{\label{tab:Kass_Raftery_Table}Kass and Raftery BIC Difference Reference Table}}
		{
		\begin{tabular}{ c|c}
		$BIC_{i}-BIC_{min}$ & Evidence Against Model $i$\\
		 \hline
		 $0 - 2$ & Not worth more than a bare mention\\
		 $2 - 6$ & Positive\\
		 $6 - 10$ & Strong\\
		 $> 10$ & Very Strong\\
		 \Xhline{3\arrayrulewidth}
		\end{tabular}
		}
		\end{table}
		This table provides a recommendation similar to the one presented for AIC in that models within 2 BIC of the minimum BIC model are suggested to be in just as much contention for selection as
		the minimum BIC model. These BIC cutoffs were motivated using an argument centered around Bayes factors (Jeffreys, 1961) and an approximate $t$ distribution, but still lack a degree of specificity
		when it comes to accounting for comparisons of models differing greatly in number of parameters or accounting for vast differences in sample sizes between different model selection applications.
		Methods with more probabilistic rigor, but still in the vein of identifying contender candidate models and selecting from among them, will be developed and explored later in this thesis.

		It is worth mentioning that not all likelihood-based information criteria follow the pattern of a goodness-of-fit term that can be viewed as a statistic offset by a penalty term that
		can be viewed as a constant for the model at hand. The \textit{Takeuchi information criterion} was developed as an extension of the Akaike information criterion that relaxes the
		assumption that the true generating model $g(y)$ belongs to the parametric class $\mathcal{F}(p)$ (Takeuchi, 1976). For a fitted model $f(y|\hat{\theta})$, TIC has the form
		\begin{equation*}
			TIC = -2 \log f(y|\hat{\theta}) + 2 \left[ \hat{tr} \{ J(\hat{\theta}) [I(\hat{\theta})]^{-1} \} \right] ,
		\end{equation*}
		where
		\begin{equation*}
			I(\hat{\theta}) = E \left. \left[ -\frac{\partial^2 \log f(y|\theta)}{\partial \theta \partial \theta '} \right] \right|_{\theta = \hat{\theta}}
		\end{equation*}
		and
		\begin{equation*}
			J(\hat{\theta}) = E \left. \left[ \left\{ \frac{\partial \log f(y|\theta)}{\partial \theta} \right\} \left\{ \frac{\partial \log f(y|\theta)}{\partial \theta} \right\}'  \right] \right|_{\theta = \hat{\theta}}
		\end{equation*}
		are quantities that must be estimated in order to calculate the given penalty term. As this penalty term needs to be estimated from the data, it is not a fixed constant as is the case
		for AIC, AICc, and BIC. However, the advantage of the TIC penalty term is that it is not formulated under the assumption that the true model belongs to the same class as the fitted candidate
		model. Bootstrap estimators of the TIC penalty term have also been developed (Cavanaugh and Shumway, 1997). This allows for an alternative computationally intensive approach to approximate this non-fixed penalty term.

		Finally, it is also worth noting that not all information criteria are likelihood-based, as is the case for the examples given above. The \textit{conceptual predictive statistic}, often
		referred to as $C_p$, was developed as a screening tool for linear regression analysis (Mallows, 1973). For a linear regression model the statistic has the form
		\begin{equation*}
			C_p = \frac{SS_{Res}}{\tilde{\sigma}^2} - n + 2p ,
		\end{equation*}
		where $SS_{Res}$ is the sum of squared residuals for the fitted model and $\tilde{\sigma}^2$ represents an unbiased estimator of the error variance $\sigma^2$. If the candidate model is correctly
		specified or overspecified, then one should expect $C_p$ to be approximately $p$. If the candidate model is underspecified, then $C_p$ is designed to exceed $p$. This is in contrast to AIC
		where it is simply advocated that lower values of the criterion are associated with models closer to the generating model, and where specific values have little meaning.
		
		Additionally, unlike AIC, which was derived using the Kullback-Leibler discrepancy as a motivation, $C_p$ was derived using instead the \textit{Gauss discrepancy} between the true regression model
		$g(y)$ and fitted model $f(y|\hat{\theta})$. The Gauss discrepancy can be described as the squared Euclidean distance between the true mean vector and the mean vector estimated under the fitted
		model, with no appearance of terms related to likelihoods, and serves as a way to measure the disparity between the true model and the fitted model. $C_p$ was designed as an approximately unbiased
		estimate of the expectation of the Gauss discrepancy. Thus, while $C_p$ and AIC are both discrepancy-based selection criteria, only AIC is likelihood-based.

		The above is not an exhaustive account of likelihood-based information criteria and related constructs, but should serve as sufficient background for the developments presented
		later in this thesis. Chapter 3 will develop a model selection procedure that can be used with likelihood-based information criteria with a non-random penalty term, and specific
		derivations related to its usage with AIC, AICc, and BIC will be presented.
		
		\section{Goodness-of-Fit Assessment}

		While the likelihood goodness-of-fit term found in many information criteria serves to measure conformity of the model to the data of interest, there exist other methods of
		assessing conformity of a specified model to the data. Broadly, the term \textit{goodness-of-fit} as it pertains to statistical modeling refers to the degree to which a certain model
		and its associated assumptions align with the observed data. Assessments of this property can take the form of visual diagnostics, quantitative diagnostics, or formal hypothesis tests to help
		determine whether the assumptions of a given model are met. While this section will focus on assessments of the goodness-of-fit of traditional normal linear regression models, as these are of
		chief relevance to this thesis, it is worth noting that assessments of goodness-of-fit exist for many other modeling paradigms as well, and many of the same principles apply.

		Linear regression analysis makes a number of assumptions about the data at hand. Perhaps the most basic assumption is that each outcome is independent given the set of predictors at hand.
		Additionally, it is assumed that the outcomes, conditioned on the covariates, have a mean that is a linear function of the covariates, with this property being referred to as \textit{linearity},
		with errors that are normally distributed with a mean of zero. These errors are assumed to be independent and identically distributed with a constant variance across all observations, with this
		constant variance property being referred to as \textit{homoskedasticity} (Kutner et al., 2005). One method of visually assessing the assumptions of linearity and homoskedasticity when using linear
		regression is to plot the residuals against either their corresponding predictor values or the corresponding observed values of the outcome, with these figures being called \textit{residual plots} (Miles, 2014). Across different values
		of the predictors and observed values, the residuals should not exhibit any specific pattern other than a mean of zero and constant variance if the assumptions of the linear regression model are met.
		Thus, deviations from this pattern in the form of a residual plot that exhibits curvature, or a change in spread, can indicate violations
		to linearity and homoskedasticity, respectively.

		However, this method of visual inspection carries with it certain limitations. When there are a large number of predictors present in a model, it may be impractical to visually inspect every
		possible residual plot with any degree of scrutiny. Additionally, there may be patterns in deviations that are undetectable upon visual inspection, such as a curved pattern that is less noticeable
		due to the scale of the predictors. If \textit{heteroskedasticity}, meaning the absence of homoskedasticity, is induced by covariates that have not been observed, a visual inspection will likely
		not reveal this violation of assumptions.

		Hypothesis tests for the goodness-of-fit of a linear model offer an alternative to visual methods. These tests posit a null hypothesis that the model adequately accommodates the data and that
		the associated assumptions are satisfied, with an alternative that the assumptions are violated in some fundamental way. One such test is the \textit{Breusch-Pagan test}, which posits a null hypothesis
		that a linear regression model does not violate homoskedasticity (Breusch and Pagan, 1979). This test involves performing an auxiliary linear regression on a transformation of the squared residuals from
		the candidate linear regression model against the covariates of interest. This is followed by the computation of a test statistic defined as half of the explained sum of squares of the auxiliary
		regression, a quantity that has an asymptotic chi-squared distribution under the null hypothesis of homoskedasticity. Essentially, the more of the variation in the squared residuals that is explained
		by the auxiliary regression, the more one suspects violations to homoskedasticity.

		However, one limitation of the Breusch-Pagan test is that it is only designed to detect a relationship between the covariates and the squared residuals that is linear, and thus it will not
		produce efficacious results if the heteroskedasticity present is not linear (Waldman, 1983). An alternative to the Breusch-Pagan test in this matter is the \textit{White test} for homoskedasticity,
		which shares the same null hypothesis of homoskedasticity of a linear model as the Breusch-Pagan test (White, 1980). The White test is similar to the Breusch-Pagan test in that it involves
		an auxiliary regression with the squared residuals as the outcome, but here, the regressors are all of the covariates in the original fitted model in addition to their squares and cross
		products. This produces a test statistic that is also asymptotically distributed as chi-squared under the null hypothesis, but that is sensitive to deviations to the null hypothesis in the form of
		heteroskedasticity related to the squares and cross products of regressors. Additionally, the test may indicate misspecification of the model if the cross products of certain
		regressors should be included in the model, but are not.

		The various ways in which the White test can detect misspecification bring to attention a weakness of the hypothesis test methods as opposed to methods of visual inspection.
		When a hypothesis is rejected, we can be reasonably confident that an assumption is violated; however, by simply performing the test, we do not glean much information as to how exactly the
		model may be misspecified. In the case of rejection of the White test, one may not know if we reject because cross-products should be included as covariates, or if perhaps there is heteroskedasticity
		present related to one of the covariates already present. This is in contrast to the method of visually observing residual plots wherein specific issues may be easier to identify.
		If a plot of the residuals versus a covariate exhibits a curved pattern, one may posit a new model that accounts for this curved relationship (Miles, 2014).
		However, hypothesis tests offer little direction other than that the current model exhibits lack-of-fit in some manner, and further investigation is necessary to determine next steps.

		In addition, both the White and Breusch-Pagan tests cannot detect heteroskedasticity induced by unobserved covariates, as their auxiliary regressions only use what has been observed.
		Thus, these tests can be blind to certain forms of heteroskedasticity, just as in the case of visual inspection of residual plots. Later in this thesis, a method will be developed
		that is capable of detecting this from of misspecification in addition to other forms.

		Other forms of assessment of goodness-of-fit beyond what is mentioned above can be employed. To assess the assumption that the data are normally distributed, the quantiles of the centered
		and scaled observations at hand can be compared to the quantiles of the standard normal distribution in a scatterplot, thus forming a \textit{Q-Q plot} for normality (Wilks and Gnanadesikan, 1968).
		If the two distributions whose quantiles are being compared are the same but for an additive constant, the points should approximately form a line. However, deviations from a line suggest
		greater deviations of the two distributions from one another. This technique is useful in assessing whether the data at hand appears to plausibly follow a normal distribution, and thus
		whether a normal linear regression and subsequent inference is appropriate.
		
		Hypothesis tests for the equivalence of two distributions also exist. The \textit{Kolmogorov-Smirnov test} is a tool that can be used to assess whether a hypothesized probability distribution
		fits the empirical distribution of a sample at hand (Smirnov, 1948). In the case of testing the goodness-of-fit of a linear regression, the null hypothesis is that the residuals are normally
		distributed, while the alternative is there is a departure from the normal distribution. The test is typically performed by standardizing the residuals to have a variance of 1,
		followed by performing the test to assess the goodness-of-fit of this empirical distribution in relation to the standard normal distribution (Stephens, 1974). However, like the tests for
		homoskedasticity detailed above, if the test is rejected, one needs to perform further investigation to elucidate the nature of the misspecification as rejection of the test does not inform
		as to how the null hypothesis was violated.

		The above presents a brief overview of techniques related to the assessment of the goodness-of-fit of normal linear regression models, but many other methods exist. Assessment of goodness-of-fit
		of modeling frameworks is vital in model selection so that one does not make an incorrect assumption from the outset and proceed with a parametric family that may exhibit gross lack-of-fit.
		Chapter 4 of this thesis develops a new hypothesis test for assessing the goodness-of-fit of a fitted linear regression model that leverages distributional properties of likelihood-based
		information criteria, and can be used to detect an array of different types of misspecification.

		\section{Likelihood Ratio Test}

		The likelihood principle is one of the foundational constructs of statistics and allows one to leverage the information in a sample to formulate estimates and to make inference pertaining
		to the model parameters. Estimates based on the method of maximum likelihood are often used when performing model selection, and the likelihood evaluated at
		these estimates helps to form the goodness-of-fit term in many information criteria detailed above.

		However, the value of the likelihood extends beyond parameter estimation and quantifying conformity to the data. Wilks first formulated the likelihood ratio test as a means to test the
		hypothesis that the true parameter $\theta$ is in some specifed subspace $\Theta_o$ of the entire parameter space of the model $\Theta$, with the alternative hypothesis being that
		$\theta$ is in the complementary space of $\Theta_o$ (Wilks, 1938). The form of the test statistic for the likelihood ratio test is often presented as
		\begin{equation}
			\lambda_{LR} = -2 \left[ \ell (\hat{\theta_o}) - \ell (\hat{\theta}) \right] ,
		\end{equation}
		with $\hat{\theta}$ being the maximum likelihood estimator with regards to the entire parameter space $\Theta$ and $\hat{\theta_o}$ being the maximum likelihood estimator with regards to the
		hypothesized space $\Theta_o$. It is worth noting that $\hat{\theta_o}$ may be some fixed value of the parameter if the hypothesis has been structured as such. Provided that the null hypothesis
		is true, it can be shown that $\lambda_{LR}$ converges asymptotically to being distributed as $\chi^2_{k}$, where $k$ is the difference in dimension between the spaces $\Theta$ and
		$\Theta_o$.

		The typical application of the likelihood ratio test as it relates to model selection is in the case of \textit{nested models}. If a smaller model with a more restricted parameter space is subsumed
		by a larger model with a more general parameter space, the former model is said to be nested with the latter. Nested models are intrinsic to the paradigm for the likelihood ratio test outlined above (Lewis et al., 2011).
		Commonly in a regression setting this can mean some predictors are included in the larger, non-nested model that are not included in the smaller, nested model, and thus the parameter space is
		restricted in that the effects for these predictors are fixed to be zero. Additionally, another example of nesting may be a longitudinal model that assumes an unstructured covariance structure
		that is quite malleable as compared to a nested counterpart assuming a more rigid structure such as compound symmetry.

		The likelihood ratio test allows one to probabilistically assess whether a model or a nested, simplified counterpart appears to provide a better fit. However, this paradigm of model selection possesses
		several key limitations. An obvious flaw is that the test is designed around comparing two distinct models and performing inference with regard to whether one or the other is 
		correctly specified, when in reality, many different candidate models may warrant consideration, and neither of the two models in question may themselves be correctly specified. 
		If multiple tests are performed to compare a variety of different models, issues associated with multiple comparisons may result (Lewis et al., 2011). Additionally, the notion of power, p-values, and statistical significance 
		become murky when one cannot be sure that the alternative hypothesis is true, as is the case with any likelihood ratio test conducted for the purpose of model selection (Akaike, 1974).
		Thus, while a powerful probabilistic tool, the likelihood ratio test in model selection must be employed with caution.

		The above presents a brief overview of the likelihood ratio test as it pertains to model selection. In Chapter 3 of this thesis, distributional properties of the likelihood ratio test are
		used to develop a model selection algorithm that relies on probability theory as opposed to ad hoc rules.

		\section{Robust Sandwich Variance Estimators}

		When employing likelihood theory to obtain parameter estimates, one can often leverage the properties of maximum likelihood estimators (MLEs) to produce reasonable estimates of the variance of the
		estimators, and thus perform inference (Millar, 2011). However, these properties are only guaranteed to hold when the model is properly specified, and poor estimates may result when assumptions
		do not hold. This has given rise to \textit{robust variance estimators} that rely on fewer assumptions than classical likelihood theory, yet can still quantify the variability of a statistic
		at hand.

		Huber first provided justifications for consistency and asymptotic normality of maximum likelihood estimators under conditions weaker than had been previously shown (Huber, 1967). White
		expanded upon this notion by deriving covariance matrix estimates for maximum likelihood estimators that are robust to a variety of different types of misspecification and only assume
		conditional independence and certain other regularity conditions (White, 1980). Defining $\theta_*$ as the \textit{pseudo-true} parameter and $\hat{\theta}_n$ as the MLE for a sample of size
		$n$, White showed the asymptotic relation
		\begin{equation*}
			\sqrt{n} (\hat{\theta}_n - \theta_*) \xrightarrow[]{d} N(0, C(\theta_* ) ) .
		\end{equation*}
		The large sample covariance matrix $C(\theta)$ can be defined using the matrices 
		\begin{equation*}
			A(\theta) = E \left[ \frac{\partial^2 f(y_t,\theta)}{\partial \theta_i \partial \theta_j} \right] 
		\end{equation*}
		and
		\begin{equation*}
			B(\theta) = E \left[ \frac{\partial f(y_t,\theta)}{\partial \theta_i} \frac{\partial f(y_t,\theta)}{\partial \theta_j} \right] 
		\end{equation*}
		where $i = 1,...,p$ and $j = 1,...,p$ respectively. Note that these matrices are akin to the expectation of the Hessian matrix with respect to the log-likelihood and the expectation of
		the outer product of the score vector. These matrices then combine to define $C(\theta)$ as
		\begin{equation*}
			C(\theta) = A(\theta)^{-1} B(\theta) A(\theta)^{-1} ,
		\end{equation*}
		and thus evaluating this quantity at $\theta_*$, one arrives at the robust asymptotic variance estimate. The resulting estimator, and those that are similar in form, are often called \textit{sandwich}
		variance estimators due to one quantity being sandwiched in between two identical others to form the statistic.

		As in general one will not know the pseudo-true parameter $\theta_*$, White indicates that the matrices
		\begin{equation*}
			A_n(\theta) = \frac{1}{n} \sum_{t=1}^{n} \frac{\partial^2 f(y_t,\theta)}{\partial \theta_i \partial \theta_j}
		\end{equation*}
		and
		\begin{equation*}
			B_n(\theta) = \frac{1}{n} \sum_{t=1}^{n} \frac{\partial f(y_t,\theta)}{\partial \theta_i} \frac{\partial f(y_t,\theta)}{\partial \theta_j} 
		\end{equation*}
		can be calculated from the data and evaluated at the MLE to form
		\begin{equation*}
			C_n(\hat{\theta}) = A_n(\hat{\theta})^{-1} B_n(\hat{\theta}) A_n(\hat{\theta})^{-1} .
		\end{equation*}
		It can then be shown that 
		\begin{equation*}
			C_n(\hat{\theta}_n) \xrightarrow[]{a.s.} C(\theta_* ) .
		\end{equation*}
		Thus, we have a variance estimator for the MLE that is both robust to much misspecification and can be calculated using the data, but yet also will be approximately equivalent to the standard
		likelihood theory estimator if the model is correctly specified. This sandwich estimator, and others like it, can be used to perform inference related to the parameters if one calls into question
		the strong assumptions involved in using the traditional maximum likelihood estimator. However, if these assumptions cannot be met and the model appears to be misspecified, one may ponder
		the merit of performing inference on the pseudo-true parameters in the first place (Freedman, 2006). Therefore, robust variance estimators serve as a hedge against slight deviances from a model
		being correctly specified, not as a tool that remediates poor model selection.

		For the purposes of this thesis, the White sandwich estimator will be used to formulate a hypothesis test to assess the goodness-of-fit of a linear regression model. Chapter 4 will detail how
		the test is to be formulated and will explicitly demonstrate the form of the sandwich estimator that is to be used.

		\section{The Bootstrap Procedure}

		The above sections detail several cases in which potentially stringent assumptions need to be met for procedures to have the desired properties. In this section, the \textit{bootstrap procedure} will
		be discussed, a method that often requires far fewer parametric assumptions.
		
		The term \textit{bootstrap} was first used by Efron to describe a procedure through which one resamples with replacement from the sample at hand to produce a new sample that will behave as if it
		was drawn from the target population (Efron, 1979). A statistic of interest can be calculated using this new sample. This resampling and subsequent calculation can be done many times to obtain many statistics that form an
		\textit{empirical bootstrap distribution} for the statistic that, assuming a sufficient sample size and an original sample that reflects the underlying distribution, will asymptotically approach the
		true sampling distribution of the statistic. With such a bootstrap distribution, one can use a variety of methods to construct confidence intervals and perform inference with regards to population
		parameters of interest. One such method of forming a confidence interval is called the \textit{percentile bootstrap}, wherein if an interval with coverage probability $100*(1-\alpha) \%$ is
		desired, the $\alpha/2$ and $1-\alpha/2$ percentiles of the empirical bootstrap distribution are used as the bounds of the interval (Efron and Tibshirani, 1993). This method is general and
		can be applied in a similar manner with little modification for the parameter or statistic of interest.

		The bootstrap procedure has several advantages, with the first being that it makes few assumptions about the distribution of the data at hand. This is in contrast to many other methods of
		statistical inference which require distributional assumptions to be met for desirable properties to hold. A second is that in addition to not needing to assume a sampling distribution for the statistic
		of interest, one need not even know the form of the sampling distribution at all in order to perform inference. This in addition to the simplicity and generalizability of the bootstrap procedure
		make it a powerful tool in a vast array of circumstances  (Efron, 2003).

		However, the bootstrap does have certain limitations which need to be accounted for to avoid misuse. The bootstrap distribution is generally only guaranteed to converge to the true sampling
		distribution in an asymptotic sense, and thus caution must be taken in finite sample size (Hinkley, 1994). Additionally, the bootstrap procedure, like nearly any other statistical method,
		is only as good as the sample one has; if the sample is biased or not representative of the true population in some other way, then the bootstrap will lead to incorrect conclusions. It is also
		worth noting that the bootstrap is more computationally intensive than many other methods, and this computational complexity can scale as more bootstrap iterations are desired or the statistic
		of interest grows in computational complexity (Efron and Tibshirani, 1993). If one keeps these limitations in mind, the bootstrap procedure can be used in a wide variety of settings with few
		other assumptions to be made.

		Variations and extensions of the bootstrap procedure detailed above have been developed. The method previously mentioned, involving resampling with replacement from the original sample, is sometimes
		referred to as the \textit{non-parametric bootstrap} in contrast to the following methods. The \textit{parametric bootstrap} involves generating a sample from a fitted model many times and calculating
		a statistic, thus producing an empirical distribution for a statistic of interest (Dekking, 2005). This method assumes that generating data from the fitted model provides a good approximation to drawing from the
		population, but can be useful when a certain model need be assumed. The \textit{residual bootstrap} involves resampling with replacement the residuals from a regression model, and then adding
		these back onto the original fitted values $\hat{y}$ to form new observations while retaining the original covariates associated with those fitted values (Efron and Tibshirani, 1993). This procedure
		is useful in that the explanatory variables remain fixed and are not assumed to be random while the outcome itself will vary in each resampling iteration.

		The bootstrap procedure will be used in Chapter 4 of this thesis to assist in the development of a general goodness-of-fit test to be used on a fitted linear model. The bootstrap allows this procedure
		to be performed despite lack of knowledge about the sampling distribution of the statistic at hand.



		